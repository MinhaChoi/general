{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "531f776f",
   "metadata": {},
   "source": [
    " # action별 sentiment, importance 산출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5358894a",
   "metadata": {},
   "source": [
    "마이크로세그먼트에서 LDA로 도출된 각 action 별로 sentiment와 importance를 구함 (sentiment구하는데 오래걸림)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00b4a2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm #작업진행률 표시\n",
    "from konlpy.tag import Okt #okt 형태소 분석기\n",
    "okt = Okt()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer #vectorizer \n",
    "from sklearn.decomposition import LatentDirichletAllocation #lda\n",
    "import numpy as np\n",
    "\n",
    "import json #json 파일 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b9250b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_okt_for_sentiment(text): #okt 전처리 함수 정의\n",
    "    #     text = spacing(text) # 띄어쓰기 보정 \n",
    "    pos_words = okt.pos(text, stem=True)\n",
    "    words = [word for word, tag in pos_words if tag in ['Noun', 'Adjective', 'Verb', 'KoreanParticle', 'VerbPrefix'] ]\n",
    "    stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다','요']\n",
    "    stopped_words = [w for w in words if not w in stopwords]\n",
    "    x ='' \n",
    "    for t in range(len(stopped_words)): #\n",
    "        x = x+' '+stopped_words[t]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a993ecb",
   "metadata": {},
   "source": [
    "### 계산값 도출할 함수 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f05cff",
   "metadata": {},
   "source": [
    "- 1. token(단어)의 감성점수 도출\n",
    "- 2. 1을 가지고 sentence(문장)의 감성점수 도출\n",
    "- 3. 2를 가지고 document(문단)의 감성점수 도출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fac526b",
   "metadata": {},
   "source": [
    "#### 1. token(단어)의 감성점수 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6bc1c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#감성점수 계산 함수, 문장 평가 함수 정의\n",
    "\n",
    "'sentiment'\n",
    "\n",
    "def score_word(token): #token의 감성점수를 계산 -2(부정) ~ 2점 (긍정)\n",
    "    with open('SentiWord_info.json', encoding='utf-8-sig', mode='r') as f: # KNU 감성사전\n",
    "        data = json.load(f)\n",
    "    result = ['None','None']\n",
    "    for i in range(0, len(data)):\n",
    "        if data[i]['word'] == token:\n",
    "            result.pop()\n",
    "            result.pop()\n",
    "            result.append(data[i]['word_root'])\n",
    "            result.append(data[i]['polarity'])\n",
    "    return result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b2ab3513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['나', '는', '하루', '종일', '기분', '이', '슬프다', '안좋다', '못', '하다', '않다', '좋다'],\n",
       " ['None',\n",
       "  'None',\n",
       "  'None',\n",
       "  'None',\n",
       "  'None',\n",
       "  'None',\n",
       "  '-2',\n",
       "  'None',\n",
       "  '0',\n",
       "  'None',\n",
       "  'None',\n",
       "  '2'])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#위 함수 도출 예시\n",
    "#token에는 okt로 토큰화된 값이, test에는 감성 값이 들어감\n",
    "\n",
    "tokens = okt.pos('나는 하루종일 기분이 슬프고 안좋아 못해 않좋아', stem = True) \n",
    "token = []\n",
    "test = []\n",
    "for i in tokens:\n",
    "    token.append(i[0])\n",
    "    test.append(score_word(i[0]))\n",
    "token, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf88c61f",
   "metadata": {},
   "source": [
    "#### 1을 가지고 sentence(문장)의 감성점수 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "138c7d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 평가하는 함수 : 각 문장 평가후 문단 감성점수 도출할 것임\n",
    "# sentence = 문자열 문장\n",
    "def score_sentence(sentence):\n",
    "    tokens = okt.pos(sentence, stem = True) #okt pos tagging 결과물 \"안녕하세요\" -> '안녕하다', a'djective'\n",
    "    \n",
    "    token = [] #okt로 분리된 토큰이 들어감\n",
    "    test = [] #토큰의 감성점수가 들어감\n",
    "    for i in tokens:\n",
    "        token.append(i[0]) \n",
    "        test.append(score_word(i[0]))\n",
    "    \n",
    "    \n",
    "    #비지도방식 감성분석에서 문제되는 부분 해결\n",
    "    #안좋다 -> 부정이 되는데 '안 좋다'의 경우 안 -> 중립 좋다-> 긍정으로 나옴\n",
    "    #따라서 위의 경우를 직접 부정으로 바꿔줌\n",
    "    #안 좋다 (짧은 부정문) 좋지 않다 (긴 부정문)\n",
    "    \n",
    "    prenegative = [\"안\", \"못\"] #짧은 부정문 처리\n",
    "    sunegative = [\"않다\", \"모르다\"] #긴 부정문 처리\n",
    "    \n",
    "    adjust = []\n",
    "    #enumerate는 for 루프를 돌릴 때, 인덱스와 원소를 동시에 얻음\n",
    "    for a, i in enumerate(test): # i에는 각 토큰이 나오고, a에는 인덱스가 나옴\n",
    "        try: # 첫번째 토큰일 경우 [a-1]이 없으므로 에러가 떠서 try처리\n",
    "            if token[a - 1] in prenegative: #'안 좋다'가 원본일 때 '좋다'를 보고있는 차례에서 앞에 '안' 이 있으면 부정으로\n",
    "                adjust.append(int(i) * -1)\n",
    "            else:\n",
    "                adjust.append(i)\n",
    "        except:\n",
    "            adjust.append(i)\n",
    "            \n",
    "    adjust2 = []\n",
    "    for a, i in enumerate(adjust):        \n",
    "        try: \n",
    "            if token[a + 1] in sunegative: #'좋지 않다'가 원본일 때 '좋다'를 보고있는 차례에서 뒤에 '않다'가 있으면 부정으로 \n",
    "                adjust2.append(int(i) * -1)\n",
    "            else:\n",
    "                adjust2.append(i)\n",
    "        except:\n",
    "            adjust2.append(i)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            adjust2.remove(\"None\") #계산값이 None으로 나온 건 제거 #숫자 계산에 글자가 나오면 안됨\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "    test2 = [] #위에 none code 제거에서 빈리스트 등장하는 경우가 있어서 test2로 다시 append해줌\n",
    "    for i in adjust2:\n",
    "        test2.append(int(i)) \n",
    "        \n",
    "    try:\n",
    "        # 이중문장, 이어진 문장의 경우 '사과는 좋은데 바나나는 싫어' 감성점수 주기 애매\n",
    "        # 선행연구에서 서로 다른 감정이 동시에 존재할 때 해당 문장을 대표하는 핵심 감정이 무엇이냐를 추출하기 위해서는 \n",
    "        # 마지막 감성어에 3배 정도의 가중치를 주는 것이 좋다\n",
    "        # 따라서, 가장 뒤에있는 감정에 가중치 3을 곱함\n",
    "        test2[-1] = test2[-1] * 3\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        return sum(test2)/len(test2) #한 문장안에 여러개의 토큰들이 갖는 감성점수를 토큰의 갯수로 나눠서 평균으로 문장 점수 출력\n",
    "    except: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f45551",
   "metadata": {},
   "source": [
    "#### 2를 가지고 document(문단)의 감성점수 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80818558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 앞뒤 문장을 고려하여 해당 키워드를 포함하는 문장을 감성분석하는 함수\n",
    "# 문장의 context를 반영한 감성점수를 뽑기 위해서 해당 문장 앞뒤에 있는 context를 구해서 감성점수를 내자\n",
    "\n",
    "# df는 SNA 단계의 결과로 나온 텍스트 데이터\n",
    "# keyword는 문자열 단어\n",
    "\n",
    "def keyword_context_sentiment(df, keyword):\n",
    "    \n",
    "    reg_keyword = '\\s{}\\s|\\s{}$|^{}\\s'.format(keyword,keyword,keyword)\n",
    "    \n",
    "    #reg_keyword 가지고 있으면 target으로\n",
    "    target = df.document_sentence_for_senti[df.document_sentence_for_senti.str.contains(reg_keyword,regex=True)].to_list() \n",
    "\n",
    "    target = list(set(target)) #중복 제거\n",
    "    result = []\n",
    "    dexx = []\n",
    "    \n",
    "    #if문 이유는 키워드를 포함하는 문장이 어떤 문단에 포함돼있냐에 따라 다른 점수가 나올 수 있음\n",
    "    if len(target) == 0: # 키워드 포함하는 문장이 없는 경우\n",
    "        dexx.append(keyword)\n",
    "        result.append(0)\n",
    "        \n",
    "        final_result = {\"index\" : dexx, \"sentiment\" : result}\n",
    "        final_result = pd.DataFrame(final_result)\n",
    "    \n",
    "    else: # 키워드 포함하는 문장이 하나일 경우\n",
    "        for i in target:\n",
    "            \n",
    "            gram = df[df[\"document_sentence_for_senti\"] == i] \n",
    "            \n",
    "            \n",
    "            if len(gram) == 1: #하나만 나올 때는 그냥 계산\n",
    "                dex = int(gram.index[0]) #인덱스 변수 설정\n",
    "                \n",
    "                if dex == df.index[0]: # 맨 앞 인덱스 #맨 앞 문단의 맨 앞 문장 경우 고려 [dex-1]이 정의되지 않아 오류 발생. 차단\n",
    "\n",
    "                    if df.loc[dex][\"document_num\"] != df.loc[dex + 1][\"document_num\"]:\n",
    "                        consider = df.loc[dex][\"document_sentence_for_senti\"]\n",
    "                    else:\n",
    "                        consider = df.loc[[dex, dex + 1]][\"document_sentence_for_senti\"]\n",
    "\n",
    "                elif dex == df.index[-1]: #맨 뒤 인덱스 #맨 뒤 문단의 맨 뒤 문장 고려 [dex+1]이 정의되지 않아 오류 발생. 차단\n",
    "                    \n",
    "                    if df.loc[dex - 1][\"document_num\"] != df.loc[dex][\"document_num\"]:\n",
    "                        consider = df.loc[dex][\"document_sentence_for_senti\"]\n",
    "                    else:\n",
    "                        consider = df.loc[[dex - 1, dex]][\"document_sentence_for_senti\"]\n",
    "                \n",
    "                elif df.loc[dex - 1][\"document_num\"] != df.loc[dex][\"document_num\"]:\n",
    "                    #해당 문장이 문단에 맨 앞인지 맨 뒤인지를 고려한 것\n",
    "                    #해당 문장이 맨 앞 문장이면 인덱스가 앞으로 가면 문단 정보가 바뀜\n",
    "                    if df.loc[dex][\"document_num\"] != df.loc[dex + 1][\"document_num\"]:\n",
    "                        consider = df.loc[dex][\"document_sentence_for_senti\"]\n",
    "                    else:\n",
    "                        consider = df.loc[[dex, dex + 1]][\"document_sentence_for_senti\"]\n",
    "                        \n",
    "                elif df.loc[dex][\"document_num\"] != df.loc[dex + 1][\"document_num\"]:\n",
    "                    #해당 문장이 맨 뒤 문장이면 인덱스가 뒤로 가면 문단 정보가 바뀜\n",
    "                    consider = df.loc[[dex - 1, dex]][\"document_sentence_for_senti\"]\n",
    "                    \n",
    "                else: # 위에 경우가 모두 아니면 해당 문장, 앞, 뒤를 모두 고려한 점수\n",
    "                    consider = df.loc[[dex - 1, dex, dex + 1]][\"document_sentence_for_senti\"]\n",
    "\n",
    "                    \n",
    "                try:\n",
    "                    consider = pd.DataFrame(consider) #1문단 1문장일 경우 \n",
    "                except:\n",
    "                    consider = pd.DataFrame([consider]) #1문단 여러 문장일 경우 (문장이 중간에 있어서 앞뒤 계산이 되는 경우)\n",
    "\n",
    "                consider.columns = [\"document_sentence_for_senti\"]\n",
    "\n",
    "                sentscore = 0\n",
    "                for i in consider[\"document_sentence_for_senti\"]:\n",
    "                    sentscore += score_sentence(i)\n",
    "                result.append(sentscore/len(consider))  # 키워드가 포함된 문장 앞뒤 문장 각각의 문장 감성점수의 평균\n",
    "                dexx.append(dex)\n",
    "\n",
    "                \n",
    "            else: # 키워드를 포함한 문장이 2개 이상일때 #리스트로 묶어서 동시에 고려\n",
    "                for dex in list(gram.index): \n",
    "                    \n",
    "                    if dex == df.index[0]: # 0번 인덱스\n",
    "                        if df.loc[dex][\"document_num\"] != df.loc[dex + 1][\"document_num\"]:\n",
    "                            consider = df.loc[dex][\"document_sentence_for_senti\"]\n",
    "                        else:\n",
    "                            consider = df.loc[[dex, dex + 1]][\"document_sentence_for_senti\"]\n",
    "\n",
    "                    elif dex == df.index[-1]: #맨뒤 인덱스\n",
    "                        if df.loc[dex - 1][\"document_num\"] != df.loc[dex][\"document_num\"]:\n",
    "                            consider = df.loc[dex][\"document_sentence_for_senti\"]\n",
    "                        else:\n",
    "                            consider = df.loc[[dex - 1, dex]][\"document_sentence_for_senti\"]\n",
    "                    \n",
    "                    elif df.loc[dex - 1][\"document_num\"] != df.loc[dex][\"document_num\"]:\n",
    "                        if df.loc[dex][\"document_num\"] != df.loc[dex + 1][\"document_num\"]:\n",
    "                            consider = df.loc[dex][\"document_sentence_for_senti\"]\n",
    "                        else:\n",
    "                            consider = df.loc[[dex, dex + 1]][\"document_sentence_for_senti\"]\n",
    "                            \n",
    "                    elif df.loc[dex][\"document_num\"] != df.loc[dex + 1][\"document_num\"]:\n",
    "                        consider = df.loc[[dex - 1, dex]][\"document_sentence_for_senti\"]\n",
    "                        \n",
    "                    else:\n",
    "                        consider = df.loc[[dex - 1, dex, dex + 1]][\"document_sentence_for_senti\"]\n",
    "\n",
    "                    try:\n",
    "                        consider = pd.DataFrame(consider)\n",
    "                        \n",
    "                    except:\n",
    "                        consider = pd.DataFrame([consider])\n",
    "\n",
    "                    consider.columns = [\"document_sentence_for_senti\"]\n",
    "\n",
    "                    sentscore = 0\n",
    "                    \n",
    "                    for i in consider[\"document_sentence_for_senti\"]: \n",
    "                        sentscore += score_sentence(i)\n",
    "                        \n",
    "                    result.append(sentscore/len(consider))\n",
    "                    dexx.append(dex)\n",
    "            \n",
    "        final_result = {\"index\" : dexx, \"sentiment\" : result}\n",
    "        final_result = pd.DataFrame(final_result)\n",
    "\n",
    "    return final_result.sentiment.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30b5059",
   "metadata": {},
   "source": [
    "### 데이터 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9b88b0",
   "metadata": {},
   "source": [
    "[cluster0_ms_micro1.csv 설명]\n",
    "\n",
    "- 명품세탁 키워드로 수집한 ward clustering 1 에 마이크로세그먼트 1 에 할당된 원문임\n",
    "- documnet_num : 행이 문장 단위로 쪼개져 있어서 문단 번호를 보고 같은 번호끼리 하나의 문단이었음을 확인\n",
    "- micro_cluster : 마이크로세그먼트 1\n",
    "- doc_cluster_0 : 명품세탁 ward clustering 1에서 마이크로세그먼트가 4개로 분리됐는데 그 중 0에 할당될 가중치\n",
    "- doc_cluster_1 : 1에 할당될 가중치(SNA 후 원문 복구할때 도출된 계산), 여기서는 doc_cluster_1이 0,2,3보다 큼\n",
    "- doc_cluster_2 : 2에 할당될 가중치\n",
    "- doc_cluster_3 : 3에 할당될 가중치\n",
    "- document_sentence : 문장 원본"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "369e76cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_num</th>\n",
       "      <th>micro_cluster</th>\n",
       "      <th>doc_cluster_0</th>\n",
       "      <th>doc_cluster_1</th>\n",
       "      <th>doc_cluster_2</th>\n",
       "      <th>doc_cluster_3</th>\n",
       "      <th>document_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>14.591668</td>\n",
       "      <td>38.712945</td>\n",
       "      <td>19.029572</td>\n",
       "      <td>26.170874</td>\n",
       "      <td>명품스니커즈 황변제거 명품일수록 좋은재질이다오늘작업한  스니커즈는 가격대가 꽤나갑니...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>14.591668</td>\n",
       "      <td>38.712945</td>\n",
       "      <td>19.029572</td>\n",
       "      <td>26.170874</td>\n",
       "      <td>히지만 원단에 필링까지 생기니 말이죠작업을더 해보려 하였으나  워낙고가에신품이라 여...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>14.591668</td>\n",
       "      <td>38.712945</td>\n",
       "      <td>19.029572</td>\n",
       "      <td>26.170874</td>\n",
       "      <td>내가 빼줬잖아  돈 받으면 반땅 명품 황변 제거 방법 얘기 해 주시면 복 아낌없이 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>14.591668</td>\n",
       "      <td>38.712945</td>\n",
       "      <td>19.029572</td>\n",
       "      <td>26.170874</td>\n",
       "      <td>아 계좌번호  올려드려야겟죠  사장님</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>129.149868</td>\n",
       "      <td>182.991209</td>\n",
       "      <td>10.467515</td>\n",
       "      <td>107.835749</td>\n",
       "      <td>명품패딩  캐나다구스 세탁표시 바로 이해하기 캐나다 구스 품질표시입니다아래 세탁기호...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3874</th>\n",
       "      <td>2827</td>\n",
       "      <td>1</td>\n",
       "      <td>111.134678</td>\n",
       "      <td>115.567792</td>\n",
       "      <td>33.466962</td>\n",
       "      <td>41.192188</td>\n",
       "      <td>공공장소에서는 아닌것 같네요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3875</th>\n",
       "      <td>2827</td>\n",
       "      <td>1</td>\n",
       "      <td>111.134678</td>\n",
       "      <td>115.567792</td>\n",
       "      <td>33.466962</td>\n",
       "      <td>41.192188</td>\n",
       "      <td>저흰 각자 속옷 가져가서 집에서 세탁 건조해 오는데요이상하신 분이네요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3876</th>\n",
       "      <td>2827</td>\n",
       "      <td>1</td>\n",
       "      <td>111.134678</td>\n",
       "      <td>115.567792</td>\n",
       "      <td>33.466962</td>\n",
       "      <td>41.192188</td>\n",
       "      <td>속옷은 본인이 빨아서 널어야지 참상식 이하이네요몰래 버리세요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3877</th>\n",
       "      <td>2827</td>\n",
       "      <td>1</td>\n",
       "      <td>111.134678</td>\n",
       "      <td>115.567792</td>\n",
       "      <td>33.466962</td>\n",
       "      <td>41.192188</td>\n",
       "      <td>우린 조리사가 위생복하고 속옷을 같이 돌리네요자기가 입고다니는 옷도 여러벌 빨아서 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3878</th>\n",
       "      <td>2833</td>\n",
       "      <td>1</td>\n",
       "      <td>1.921466</td>\n",
       "      <td>36.749894</td>\n",
       "      <td>13.904209</td>\n",
       "      <td>11.365785</td>\n",
       "      <td>연 베이지 계열  명품라인   카 곰돌이 방수 옥스포드 가정용 가능수급 사진의 색상...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3879 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      document_num  micro_cluster  doc_cluster_0  doc_cluster_1  \\\n",
       "0               15              1      14.591668      38.712945   \n",
       "1               15              1      14.591668      38.712945   \n",
       "2               15              1      14.591668      38.712945   \n",
       "3               15              1      14.591668      38.712945   \n",
       "4               25              1     129.149868     182.991209   \n",
       "...            ...            ...            ...            ...   \n",
       "3874          2827              1     111.134678     115.567792   \n",
       "3875          2827              1     111.134678     115.567792   \n",
       "3876          2827              1     111.134678     115.567792   \n",
       "3877          2827              1     111.134678     115.567792   \n",
       "3878          2833              1       1.921466      36.749894   \n",
       "\n",
       "      doc_cluster_2  doc_cluster_3  \\\n",
       "0         19.029572      26.170874   \n",
       "1         19.029572      26.170874   \n",
       "2         19.029572      26.170874   \n",
       "3         19.029572      26.170874   \n",
       "4         10.467515     107.835749   \n",
       "...             ...            ...   \n",
       "3874      33.466962      41.192188   \n",
       "3875      33.466962      41.192188   \n",
       "3876      33.466962      41.192188   \n",
       "3877      33.466962      41.192188   \n",
       "3878      13.904209      11.365785   \n",
       "\n",
       "                                      document_sentence  \n",
       "0     명품스니커즈 황변제거 명품일수록 좋은재질이다오늘작업한  스니커즈는 가격대가 꽤나갑니...  \n",
       "1     히지만 원단에 필링까지 생기니 말이죠작업을더 해보려 하였으나  워낙고가에신품이라 여...  \n",
       "2     내가 빼줬잖아  돈 받으면 반땅 명품 황변 제거 방법 얘기 해 주시면 복 아낌없이 ...  \n",
       "3                                  아 계좌번호  올려드려야겟죠  사장님  \n",
       "4     명품패딩  캐나다구스 세탁표시 바로 이해하기 캐나다 구스 품질표시입니다아래 세탁기호...  \n",
       "...                                                 ...  \n",
       "3874                                    공공장소에서는 아닌것 같네요  \n",
       "3875             저흰 각자 속옷 가져가서 집에서 세탁 건조해 오는데요이상하신 분이네요  \n",
       "3876                  속옷은 본인이 빨아서 널어야지 참상식 이하이네요몰래 버리세요  \n",
       "3877  우린 조리사가 위생복하고 속옷을 같이 돌리네요자기가 입고다니는 옷도 여러벌 빨아서 ...  \n",
       "3878  연 베이지 계열  명품라인   카 곰돌이 방수 옥스포드 가정용 가능수급 사진의 색상...  \n",
       "\n",
       "[3879 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터 불러오기\n",
    "\n",
    "d01 = pd.read_csv(\"cluster0_ms_micro1.csv\", index_col=0)\n",
    "#d02 = pd.read_csv(\"cluster0_ms_micro2.csv\")\n",
    "#d10 = pd.read_csv(\"cluster1_ms_micro0.csv\")\n",
    "#d11 = pd.read_csv(\"cluster1_ms_micro1.csv\")\n",
    "#d12 = pd.read_csv(\"cluster1_ms_micro2.csv\")\n",
    "\n",
    "#dd = [d01,d02,d10,d11,d12]\n",
    "#tt = [4,2,7,4,3]\n",
    "\n",
    "dd = [d01] #SNA후 원문 복구된 파일 dataset\n",
    "tt = [4] #앞에 LDA 코드에서 선정된 토픽갯수가 여기에 들어감 \n",
    "\n",
    "formemo =['명품세탁 0_1번 4개토픽'] #각 마이크로 세그먼트마다 토픽 갯수 다름(각 데이터마다 토픽 갯수 직접 판단)\n",
    "          #,'명품세탁 0_2번 2개토픽','명품세탁 1_0번 7개토픽','명품세탁 1_1번 4개토픽','명품세탁 1_2번 3개토픽']\n",
    "\n",
    "d01.reset_index(inplace=True, drop=True) #인덱스 재설정\n",
    "d01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2402004a",
   "metadata": {},
   "source": [
    "[과정]\n",
    "\n",
    "1. 여러 dataset 한번에 처리하려고 for문에서 시작\n",
    "2. 문장 단위로 쪼개져 있던 데이터 문단 단위로 변환\n",
    "3. LDA에서 만들어지는 행렬 등을 사용하기 위해서 LDA 진행\n",
    "4. importance 계산\n",
    "5. sentiment 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c82d8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3879/3879 [00:05<00:00, 695.75it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5756/5756 [17:34<00:00,  5.46it/s]\n"
     ]
    }
   ],
   "source": [
    "for order in range(len(dd)): \n",
    "    \n",
    "    memo = formemo[order] #여러개 한번에 처리할때 순서 지정\n",
    "    df = dd[order]\n",
    "    TOPIC_NUM = tt[order]\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    #문장 단위로 쪼개져 있던 데이터 문단 단위로 변환\n",
    "    for i in df[\"document_num\"].unique(): # 순수 문단번호\n",
    "        result = \"\" \n",
    "        for w in df[df[\"document_num\"] == i].values: \n",
    "            result += w[-1] # 맨뒤가 document_sentence임\n",
    "            result += \" \" # document_sentence가 서로 띄어쓰기 되도록\n",
    "        data.append(result)\n",
    "    data = pd.DataFrame(data, columns = [\"moondan\"])\n",
    "    data.index = df[\"document_num\"].unique()\n",
    "    data = data.dropna() #문단단위로 변환된 데이터\n",
    "\n",
    "    okt_pos = []\n",
    "    for i in tqdm(range(len(df))):\n",
    "        okt_pos.append(preprocess_okt_for_sentiment(df['document_sentence'].iloc[i]))\n",
    "    df['document_sentence_for_senti'] = okt_pos \n",
    "    df.document_sentence_for_senti  # LDA의 토픽 키워드를 매핑시켜서 키워드별 감성점수 구할 sentence들\n",
    "\n",
    "    \n",
    "    #정해진 topic num 으로 LDA 진행\n",
    "    #LDA로 도출되는 정보들을 사용해야하기 때문\n",
    "    tf_vect = TfidfVectorizer(tokenizer=preprocess_okt_for_sentiment, ngram_range=(2,3), min_df=5, max_df=20000) # n_gram 설정\n",
    "    dtm = tf_vect.fit_transform(data['moondan']) #LDA \n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=TOPIC_NUM, random_state=0, learning_method='batch', max_iter=20)\n",
    "    lda.fit(dtm)\n",
    "    # print(dtm)  \n",
    "\n",
    "    topic_document_matrix = lda.transform(dtm).transpose() # topic_document_matrix\n",
    "\n",
    "    \n",
    "    #importance : 토픽별 비중 계산\n",
    "    \n",
    "    'importance'\n",
    "    imp = []\n",
    "    for i in range(topic_document_matrix.shape[0]): # 토픽마다 돌아가면서\n",
    "        imp.append(sum(topic_document_matrix[i])) #토픽_도큐먼트 행렬 가로기준으로 합\n",
    "    imp = imp/sum(imp) # 전체 토픽 분의 각 토픽의 비중\n",
    "    imp\n",
    "\n",
    "\n",
    "    # topic_keyword_matrix 각 키워드 평균 감성점수랑 내적할 벡터\n",
    "    # tf_vect.get_feature_names()  이거랑 각 topic_keyword_matrix의 순서가 같음\n",
    "\n",
    "    topic_keyword_matrix  = lda.components_ # topic-keyword matrix  행이 토픽, 열이 바이,트라이그램\n",
    "\n",
    "    keyword_sentiment_vector = []\n",
    "    for keyword in tqdm(tf_vect.get_feature_names()):\n",
    "        keyword_sentiment_vector.append(keyword_context_sentiment(df,keyword))\n",
    "\n",
    "    sat = []\n",
    "    for i in range(topic_keyword_matrix.shape[0]): # i 는 0~4\n",
    "        topic_keyword_vector = topic_keyword_matrix[i] # 각 토픽의 키워드 벡터\n",
    "        sat.append(np.inner(topic_keyword_vector,keyword_sentiment_vector))\n",
    "    \n",
    "    imp, sat\n",
    "    \n",
    "    f = open(\"명품세탁중요도만족도.txt\", 'a')\n",
    "    f.write(memo)\n",
    "    f.write('\\n')\n",
    "    f.write('중요도')\n",
    "    f.write(str(imp))\n",
    "    f.write('\\n')\n",
    "    f.write('만족도')\n",
    "    f.write(str(sat))\n",
    "    f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df8209f",
   "metadata": {},
   "source": [
    "=> txt 파일에 action별 중요도와 만족도 점수가 저장됨. 폴더에서 확인"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "287px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
